
                <div class="publication">
                    <img src="pub_thumbnails/a2vis_amodal_aware_approach_to_video_instance_segmentation.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://uark-aicv.github.io/A2VIS/">A2VIS: Amodal-Aware Approach to Video Instance Segmentation</a></strong><br>
                        <span class="author me">Minh Tran</span>, <span class="author">Thang Pham</span>, <span class="author">Winston Bounsavy</span>, <span class="author">Tri Nguyen</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">IMAVIS &amp;middot; 2025 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://uark-aicv.github.io/A2VIS/" class="pub-link">Project page</a>
        <a href="https://arxiv.org/pdf/2412.01147" class="pub-link">Paper</a>
        <a href="https://github.com/UARK-AICV/A2VIS" class="pub-link">Code</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/ct_scangaze_a_dataset_and_baselines_for_3d_volumetric_scanpath_modeling.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/abs/2507.12591">CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling</a></strong><br>
                        <span class="author">Trong Thang Pham</span>, <span class="author">Akash Awasthi</span>, <span class="author">Saba Khan</span>, <span class="author">Esteban Duran Marti</span>, <span class="author">Tien-Phat Nguyen</span>, <span class="author">Khoa Vo</span>, <span class="author me">Minh Tran</span>, <span class="author">Son Nguyen</span>, <span class="author">Cuong Tran</span>, <span class="author">Yuki Ikebe</span>, <span class="author">Anh Totti Nguyen</span>, <span class="author">Anh Nguyen</span>, <span class="author">Zhigang Deng</span>, <span class="author">Carol C Wu</span>, <span class="author">Hien Nguyen</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">ICCV &amp;middot; 2025 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/abs/2507.12591" class="pub-link">Paper</a>
        <a href="https://github.com/UARK-AICV/CTScanGaze" class="pub-link">Code</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/dualfit_a_two_stage_virtual_try_on_via_warping_and_synthesis.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/abs/2508.12131">DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis</a></strong><br>
                        <span class="author me">Minh Tran</span>, <span class="author">Johnmark Clements</span>, <span class="author">Annie Prasanna Manoharan</span>, <span class="author">Tri Nguyen</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">ICCV &amp;middot; 2025 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/abs/2508.12131" class="pub-link">Paper</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/open_fusion_real_time_open_vocabulary_3d_mapping_and_queryable_scene_representation.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/pdf/2310.03923">Open-fusion: Real-time open-vocabulary 3d mapping and queryable scene representation</a></strong><br>
                        <span class="author">Kashu Yamazaki</span>, <span class="author">Taisei Hanyu</span>, <span class="author">Khoa Vo</span>, <span class="author">Thang Pham</span>, <span class="author me">Minh Tran</span>, <span class="author">Gianfranco Doretto</span>, <span class="author">Anh Nguyen</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">ICRA &amp;middot; 2024 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/pdf/2310.03923" class="pub-link">Paper</a>
        <a href="https://github.com/UARK-AICV/OpenFusion" class="pub-link">Code</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/shapeformer_shape_prior_visible_to_amodal_transformer_based_amodal_instance_segmentation.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/abs/2403.11376">Shapeformer: Shape prior visible-to-amodal transformer-based amodal instance segmentation</a></strong><br>
                        <span class="author me">Minh Tran</span>, <span class="author">Winston Bounsavy</span>, <span class="author">Khoa Vo</span>, <span class="author">Anh Nguyen</span>, <span class="author">Tri Nguyen</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">IJCNN &amp;middot; 2024 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/abs/2403.11376" class="pub-link">Paper</a>
        <a href="https://github.com/UARK-AICV/ShapeFormer" class="pub-link">Code</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/amodal_instance_segmentation_with_diffusion_shape_prior_estimation.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/abs/2409.18256">Amodal Instance Segmentation with Diffusion Shape Prior Estimation</a></strong><br>
                        <span class="author me">Minh Tran</span>, <span class="author">Khoa Vo</span>, <span class="author">Tri Nguyen</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">ACCV &amp;middot; 2024 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/abs/2409.18256" class="pub-link">Paper</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/henasy_learning_to_assemble_scene_entities_for_interpretable_egocentric_video_language_model.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/abs/2406.00307">Henasy: Learning to assemble scene-entities for interpretable egocentric video-language model</a></strong><br>
                        <span class="author">Khoa Vo</span>, <span class="author">Thinh Phan</span>, <span class="author">Kashu Yamazaki</span>, <span class="author me">Minh Tran</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">NeurIPS &amp;middot; 2024 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/abs/2406.00307" class="pub-link">Paper</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/light_weight_deformable_registration_using_adversarial_learning_with_distilling_knowledge.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/abs/2110.01293">Light-weight deformable registration using adversarial learning with distilling knowledge</a></strong><br>
                        <span class="author me">Minh Tran</span>, <span class="author">Tuong Do</span>, <span class="author">Huy Tran</span>, <span class="author">Erman Tjiputra</span>, <span class="author">Quang D Tran</span>, <span class="author">Anh Nguyen</span><br>
                        <span class="conference">TMI &amp;middot; 2022 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/abs/2110.01293" class="pub-link">Paper</a>
        <a href="https://github.com/aioz-ai/LDR_ALDK" class="pub-link">Code</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/ss_3dcapsnet_self_supervised_3d_capsule_networks_for_medical_segmentation_on_less_labeled_data.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/pdf/2201.05905">SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data</a></strong><br>
                        <span class="author me">Minh Tran</span>, <span class="author">Loi Ly</span>, <span class="author">Binh-Son Hua</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">ISBI &amp;middot; 2022 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/pdf/2201.05905" class="pub-link">Paper</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/aisformer_amodal_instance_segmentation_with_transformer.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://uark-aicv.github.io/AISFormer/">AISFormer: Amodal Instance Segmentation with Transformer</a></strong><br>
                        <span class="author me">Minh Tran</span>, <span class="author">Khoa Vo</span>, <span class="author">Kashu Yamazaki</span>, <span class="author">Arthur Fernandes</span>, <span class="author">Michael Kidd</span>, <span class="author">Ngan Le</span><br>
                        <span class="conference">BMCV &amp;middot; 2022 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://uark-aicv.github.io/AISFormer/" class="pub-link">Project page</a>
        <a href="https://arxiv.org/pdf/2210.06323" class="pub-link">Paper</a>
        <a href="https://github.com/UARK-AICV/AISFormer" class="pub-link">Code</a>
                        </div>
                    </div>
                </div>


                <div class="publication">
                    <img src="pub_thumbnails/multiple_meta_model_quantifying_for_medical_visual_question_answering.png" ></img>
                    <div class="pub-info">
                        <p><strong><a href="https://arxiv.org/abs/2105.08913">Multiple meta-model quantifying for medical visual question answering</a></strong><br>
                        <span class="author">Tuong Do</span>, <span class="author">Binh X. Nguyen</span>, <span class="author">Erman Tjiputra</span>, <span class="author me">Minh Tran</span>, <span class="author">Quang D. Tran</span>, <span class="author">Anh Nguyen</span><br>
                        <span class="conference">MICCAI &amp;middot; 2021 &amp;middot;</span><br>
                        </p>
                        <div class="pub-links">
        <a href="https://arxiv.org/abs/2105.08913" class="pub-link">Paper</a>
                        </div>
                    </div>
                </div>