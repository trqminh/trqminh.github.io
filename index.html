<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Minh Tran</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="9f03ab05-8757-4ca9-a431-096d79735717" class="page serif"><header><h1 class="page-title">Minh Tran</h1><p class="page-description"></p></header><div class="page-body"><div id="92aeaa37-aae0-4596-a306-b47f8becb482" class="column-list"><div id="0db7ff75-687a-4c9d-8db8-3ca6e8fb652e" style="width:68.75%" class="column"><p id="bba97544-035c-44b9-9d9c-edd845cbfea4" class="">I am a Ph.D. student in Computer Science at the University of Arkansas, advised by <mark class="highlight-blue"><a href="https://scholar.google.com/citations?user=8ck0k_UAAAAJ&amp;hl=en">Ngan Le</a></mark>. Before starting my Ph.D., I was a research assistant at <mark class="highlight-blue"><a href="https://aioz.network/">AIOZ AI</a></mark>, advised by <mark class="highlight-blue"><a href="https://scholar.google.com/citations?user=qCcSKkMAAAAJ&amp;hl=en">Tuong Do</a></mark> and <mark class="highlight-blue"><a href="https://scholar.google.com/citations?user=gEbaF0sAAAAJ&amp;hl=en">Anh Nguyen</a></mark>. </p><p id="7554008a-4138-4ca1-bc51-801e5a616b99" class=""><mark class="highlight-blue"><strong><a href="mailto:minht@uark.edu">Email</a></strong></mark><mark class="highlight-blue"><strong> / </strong></mark><mark class="highlight-blue"><strong><a href="https://drive.google.com/file/d/131IceBPKFhjxgd6WYetqoLkrxlsNshaY/view?usp=drive_link">CV</a></strong></mark><mark class="highlight-blue"><strong> / </strong></mark><mark class="highlight-blue"><strong><a href="https://scholar.google.com/citations?user=AmQwXDUAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar</a></strong></mark><mark class="highlight-blue"><strong> / </strong></mark><mark class="highlight-blue"><strong><a href="http://github.com/trqminh">Github</a></strong></mark><mark class="highlight-blue"><strong> / </strong></mark><mark class="highlight-blue"><strong><a href="https://www.linkedin.com/in/minh-tran-a10b8b176/">Linkedin</a></strong></mark></p><nav id="148d5bc0-4478-8070-ba65-db655e0b402c" class="block-color-gray_background table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#1894e9c1-694b-4042-8ec6-be9ef74a5140">Research</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#148d5bc0-4478-801f-948b-ebd29c0e88aa">Activities</a></div></nav></div><div id="a51fd752-4919-4e22-a330-f3ccb7b3af4d" style="width:31.25%" class="column"><figure id="8fa0075a-17b0-460c-8769-f0b1401f4d49" class="image" style="text-align:center"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/me_cleanup.png"><img style="width:192px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/me_cleanup.png"/></a></figure></div></div><hr id="2f689e63-f610-4a86-adbc-258d8baa84da"/><hr id="1d658c10-f26a-4713-a1ad-53dd04fb65ff"/><h2 id="1894e9c1-694b-4042-8ec6-be9ef74a5140" class="">Research</h2><p id="6688a819-6399-4efb-8d99-0fd871a0641c" class="">My research interests are algorithms for <strong>visual generative</strong> tasks (amodal completion, image inpainting, virtual try-on, etc.); <strong>visual perception</strong> tasks (object detection, segmentation, and tracking); and <strong>vision and language interaction </strong>(VLMs, text-guided generation).</p><div id="1e5d5bc0-4478-8097-96f7-d0c7ab02acc7" class="column-list"><div id="1e5d5bc0-4478-8072-849a-c0098c7d7398" style="width:12.5%" class="column"><p id="1e5d5bc0-4478-8083-95a1-e3b31fd149a4" class=""><em><mark class="highlight-gray">2025</mark></em></p></div><div id="1e5d5bc0-4478-80d7-bb41-dabb569ccb38" style="width:87.5%" class="column"><hr id="1e5d5bc0-4478-80ee-875e-f456199d65c6"/></div></div><div id="1e5d5bc0-4478-8009-8cf3-e458797a2ebb" class="column-list"><div id="1e5d5bc0-4478-803c-a5d2-df987fcf3539" style="width:25%" class="column"><figure id="d935d6c0-832f-4570-8410-5557575a26c2" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/a2vis.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/a2vis.png"/></a></figure><p id="1e5d5bc0-4478-80d3-8e64-cc7a95b509c9" class="">
</p></div><div id="1e5d5bc0-4478-8089-ba82-d82c143e96e9" style="width:75%" class="column"><p id="540136a1-ffc7-493a-96ad-697a80c931ef" class=""><mark class="highlight-blue"><strong>A2VIS: Amodal-aware Approach to Video Instance Segmentation</strong></mark></p><p id="154d5bc0-4478-8078-bcad-cdf528fc467d" class=""><span style="border-bottom:0.05em solid">Minh Tran*</span>, Thang Pham*, Winston Bounsavy, Tri Nguyen, Ngan Le</p><p id="148d5bc0-4478-80a8-a918-d8be259d5992" class=""><em>Image and Vision Computing Journal 2025</em></p><p id="154d5bc0-4478-802f-ad17-ebea0d955342" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2412.01147">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://uark-aicv.github.io/A2VIS">project page</a></mark></p><p id="1e5d5bc0-4478-8019-ba29-f31583ec303c" class="">
</p></div></div><div id="1e5d5bc0-4478-8091-89a3-db563dffe704" class="column-list"><div id="1e5d5bc0-4478-8026-b748-cae641617117" style="width:25%" class="column"><figure id="1e5d5bc0-4478-8074-bff3-e52a21e9b255" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Screenshot_2025-04-30_at_1.27.29_PM.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Screenshot_2025-04-30_at_1.27.29_PM.png"/></a></figure></div><div id="1e5d5bc0-4478-8014-b7bb-f49b46751f46" style="width:75%" class="column"><p id="1e5d5bc0-4478-80ca-953b-c8b536e91f4d" class=""><mark class="highlight-blue"><strong>S3Former: Self-supervised High-resolution Transformer for Solar PV Profiling</strong></mark></p><p id="1e5d5bc0-4478-800f-b9c6-f6af749b4257" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Adrian De Luis, Haitao Liao, Ying Huang, Roy McCann, Alan Mantooth, Jack Cothren, Ngan Le</p><p id="1e5d5bc0-4478-8049-a202-ddfb5a8ba97f" class=""><em>IEEE Transactions on Smart Grid 2025</em></p><p id="1e5d5bc0-4478-8064-a053-dca02d940d80" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2405.04489">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://ieeexplore.ieee.org/document/10874220">paper</a></mark></p></div></div><div id="32ac80d0-74d8-4554-86f7-102c0257b26a" class="column-list"><div id="615c3230-24c2-4542-acca-2dd70ed5044b" style="width:12.5%" class="column"><p id="55eaf64b-8b2f-4703-b4db-bd50aa67c11a" class=""><em><mark class="highlight-gray">2024</mark></em></p></div><div id="2c623482-1e8f-44d5-9b5b-d3771adb65d6" style="width:87.5%" class="column"><hr id="77b6c10b-037f-4619-b42c-7a7ee54d8029"/></div></div><div id="32cf01b6-2146-4ac1-b00b-fb8232079ffd" class="column-list"><div id="4767ca0d-992c-4bd9-bc7c-e6782891df91" style="width:25%" class="column"><figure id="2ef43377-9263-4f38-8349-5416e84a38a7" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/vac.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/vac.png"/></a></figure></div><div id="5fe0caaa-e86a-4050-a94e-472a59d83ee2" style="width:75%" class="column"><p id="154d5bc0-4478-805b-b1de-e2a554c6b09a" class=""><mark class="highlight-blue"><strong>Text-Guided Video Amodal Completion</strong></mark></p><p id="154d5bc0-4478-8013-8dce-fc44840e37cc" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Winston Bounsavy, Taisei Hanyu, Thang Pham, Khoa Vo, Tri Nguyen, Ngan Le</p><p id="f2977ae1-2ff0-4846-bafb-19b00464e4a8" class=""><em>Preprint 2024</em></p><p id="154d5bc0-4478-80e2-b0c8-e820ff5c73ca" class=""><mark class="highlight-blue"><a href="https://trqminh.github.io/pdfs/vac.pdf">paper</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://uark-aicv.github.io/TextGuidedVAC">project page</a></mark></p><p id="154d5bc0-4478-80bd-8cb4-e08c67e3ac6e" class="">
</p></div></div><div id="148d5bc0-4478-80b0-a9dd-e0a165232ba4" class="column-list"><div id="50241cd1-3ee3-4a60-a267-04cef8e69143" style="width:25%" class="column"><figure id="7616c837-c2ba-4065-946c-274503b8111f" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/d82de04c-bfbf-4b79-8930-2abe42a3035e.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/d82de04c-bfbf-4b79-8930-2abe42a3035e.png"/></a></figure></div><div id="90506efa-1e12-4324-941b-cecf309dd55e" style="width:75%" class="column"><p id="154d5bc0-4478-80fb-82bd-c059a8ae6b25" class=""><mark class="highlight-blue"><strong>HENASY: Learning to Assemble Scene-Entities for Egocentric Video-Language Model</strong></mark></p><p id="154d5bc0-4478-8055-9c90-c5269397479b" class="">Khoa Vo, Thinh Phan, Kashu Yamazaki, <span style="border-bottom:0.05em solid">Minh Tran</span>, Ngan Le</p><p id="6de2340b-72fb-4ff4-a466-69e128f9ac78" class=""><em>Neurips 2024</em></p><p id="154d5bc0-4478-80c7-8807-de7b83dc3f4c" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2406.00307">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://github.com/UARK-AICV/HENASY">code</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://uark-aicv.github.io/HENASY">project page</a></mark></p><p id="154d5bc0-4478-8097-8588-fca85c7e59c9" class="">
</p></div></div><div id="148d5bc0-4478-8054-b510-ebd299bd02e8" class="column-list"><div id="8bd97f99-4711-4f3f-9621-4ac2e313c21d" style="width:25%" class="column"><figure id="c6c25826-e850-4fb1-ad06-d8a8f4bd9202" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/diffsp.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/diffsp.png"/></a></figure></div><div id="6f3894de-1ae5-4bf7-9817-f1914a059003" style="width:75%" class="column"><p id="154d5bc0-4478-8031-92a5-fd91c2be6470" class=""><mark class="highlight-blue"><strong>Amodal Instance Segmentation with Diffusion Shape Prior Estimation</strong></mark></p><p id="154d5bc0-4478-8081-9cc3-f54b6ce10e8a" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Khoa Vo, Tri Nguyen, Ngan Le</p><p id="2056e22c-07dc-41a7-9f4c-92934cdc9925" class=""><em>ACCV 2024</em></p><p id="154d5bc0-4478-805e-970d-e7ebb6fe7768" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2409.18256">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://uark-aicv.github.io/AISDiff">project page</a></mark></p><p id="154d5bc0-4478-8061-8978-dbe8cd40a4fb" class="">
</p></div></div><div id="148d5bc0-4478-802d-ad84-c54aad226245" class="column-list"><div id="72eea508-2df6-4dfd-898b-fa5532234ac9" style="width:25%" class="column"><figure id="2f213167-b5bc-4e35-b467-010264fa58ae" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled.png"/></a></figure><p id="d95b2cbe-0b20-4d22-90d5-2856390825dd" class="">
</p></div><div id="2f8869e3-8a0b-4159-bc36-13f12326f8ef" style="width:75%" class="column"><p id="b9075e95-02ed-471e-9911-19c42210e371" class=""><mark class="highlight-blue"><strong>ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation</strong></mark></p><p id="154d5bc0-4478-8053-a950-c458b09c9de4" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Winston Bounsavy, Khoa Vo, Anh Nguyen, Tri Nguyen, Ngan Le</p><p id="0b1114e3-2a7a-472d-b543-3c3a003f672d" class=""><em>IJCNN 2024</em></p><p id="154d5bc0-4478-8008-8db7-c2649119e541" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2403.11376">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://github.com/UARK-AICV/ShapeFormer">code</a></mark></p><p id="154d5bc0-4478-8071-8dc5-c90085846712" class="">
</p></div></div><div id="bc949e7c-2864-42a4-8b3e-a93137f7402f" class="column-list"><div id="f78c711a-a0a5-4b7d-974b-0c7c0ea3d3c8" style="width:25%" class="column"><figure id="9361566d-326f-4191-83ae-b77946eb71bb" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%201.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%201.png"/></a></figure></div><div id="70c5bd3c-71db-42d0-a381-2c6e7a50628d" style="width:75%" class="column"><p id="5f48ccda-1eda-4e9f-9fc7-44e13aba486d" class=""><mark class="highlight-blue"><strong>Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation</strong></mark></p><p id="154d5bc0-4478-800a-8afc-f2568073a6f7" class="">Kashu Yamazaki, Taisei Hanyu, Khoa Vo, Thang Pham, <span style="border-bottom:0.05em solid">Minh Tran</span>, Gianfranco Doretto, Anh Nguyen, Ngan Le</p><p id="a6438ec4-c4c3-4088-88b3-bacca52b2c5f" class=""><em>ICRA 2024</em></p><p id="154d5bc0-4478-8030-8a6b-d452653cab58" class=""><mark class="highlight-blue"><a href="https://arxiv.org/pdf/2310.03923">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://github.com/UARK-AICV/OpenFusion">code</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://uark-aicv.github.io/OpenFusion/">project page</a></mark></p></div></div><div id="01bce061-26ed-4c82-9089-b80cfb9f2a68" class="column-list"><div id="aa04122c-eb4f-4201-9af4-3293ed66146b" style="width:12.5%" class="column"><p id="24314d28-7460-4cee-a722-97f69bdb29ea" class=""><mark class="highlight-gray"><em>2023</em></mark></p></div><div id="a803189b-8c6a-48b4-98ac-3f866b219d7d" style="width:87.5%" class="column"><hr id="f6ba8157-3dd6-4a75-ba11-6bfd85c925b5"/></div></div><div id="148d5bc0-4478-8014-b16a-d793c5a6a79d" class="column-list"><div id="dd7ded0d-7909-47c9-8aad-81b29b6a5287" style="width:25%" class="column"><figure id="1c5c9b3a-457e-4f29-82f8-5f8b84cf3132" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%202.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%202.png"/></a></figure></div><div id="d28a2370-f666-495f-9dbd-85d9dfc4cff1" style="width:75%" class="column"><p id="68ecff93-1b16-48ae-b09d-872426d5afa7" class=""><mark class="highlight-blue"><strong>aistron: Amodal Instance Segmentation Toolbox and Benchmark</strong></mark></p><p id="154d5bc0-4478-804e-8cc2-f314332cc6df" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Ngan Le</p><p id="154d5bc0-4478-80ef-b9ff-eee7d43ce2a7" class=""><mark class="highlight-blue"><a href="http://github.com/trqminh/aistron">code</a></mark></p></div></div><div id="1d30bf76-4d3c-43ff-bc22-ca41c6abc27f" class="column-list"><div id="67b80e2f-9c9d-4f12-b42b-68597868bc69" style="width:12.5%" class="column"><p id="3af59a19-606f-4348-8605-c2753ef38cf0" class=""><mark class="highlight-gray"><em>2022</em></mark></p></div><div id="c911e29d-6cf1-48dd-bd17-074d8fbd3087" style="width:87.5%" class="column"><hr id="41ee8cf2-8534-4410-9fff-fb17871218d2"/></div></div><div id="33a5e0a6-35be-4741-8f79-bd0080b78dc8" class="column-list"><div id="001e6a71-7091-452e-a982-489990d8ee68" style="width:25%" class="column"><figure id="a0cb501c-2b56-4615-9eb6-d1136c8fbe17" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%203.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%203.png"/></a></figure></div><div id="c170cdad-37f7-4b32-b746-4c72fcf0a6e6" style="width:75%" class="column"><p id="2b0d8acb-681b-4e76-9d5b-f9e60701244a" class=""><mark class="highlight-blue"><strong>AISFormer: Amodal Instance Segmentation with Transformer</strong></mark></p><p id="154d5bc0-4478-80d7-8dfd-ca72cebe0c3e" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Khoa Vo, Kashu Yamazaki, Arthur Fernandes, Michael Kidd, Ngan Le</p><p id="148d5bc0-4478-8022-811d-d078b35767f8" class=""><em>BMVC 2022</em></p><p id="154d5bc0-4478-80bb-8865-ec356c017ab6" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2210.06323">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://github.com/UARK-AICV/AISFormer">code</a></mark><mark class="highlight-blue"> /</mark><mark class="highlight-blue"><a href="http://uark-aicv.github.io/AISFormer"> project page</a></mark></p><p id="154d5bc0-4478-80c5-9cb8-eb3d936e6ed0" class="">
</p></div></div><div id="28490124-db45-4aff-b4be-2ae13bcbb3f6" class="column-list"><div id="07b05796-e0a0-4a7a-a57c-853983b0e548" style="width:25%" class="column"><figure id="a16725ce-8467-47cd-a1b7-ff3ab513a6a3" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%204.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%204.png"/></a></figure></div><div id="4642dacb-d839-4e6f-a12c-c4b5d46f3b87" style="width:75%" class="column"><p id="9af724ef-4a2e-4a73-85ee-f5eef1498f16" class=""><mark class="highlight-blue"><strong>3DConvCaps: 3DUnet with Convolutional Capsule Encoder for Medical Image Segmentation</strong></mark></p><p id="154d5bc0-4478-8076-ae9b-ef2e56a23758" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Viet-Khoa Vo-Ho, Ngan T.H. Le</p><p id="148d5bc0-4478-80a8-8f51-cf68a9a483d5" class=""><em>International Conference on Pattern Recognition 2022</em></p><p id="154d5bc0-4478-8020-aa01-db6b80d0a6cb" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2205.09299">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://github.com/UARK-AICV/3DConvCaps">code</a></mark></p><p id="154d5bc0-4478-805d-ad26-ce11b36b4f31" class="">
</p></div></div><div id="a05c9d90-fc0c-43ff-8290-610524beba1d" class="column-list"><div id="a380d051-6f37-43d5-ab33-713502f3807c" style="width:25%" class="column"><figure id="e89a270d-9b38-49f5-9a0c-79f7d40649aa" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%205.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%205.png"/></a></figure><p id="351d27b9-7d29-4e4b-ba13-38507cb4db9a" class="">
</p></div><div id="294a98a7-94c6-4c16-933c-7a800cf7e222" style="width:75%" class="column"><p id="1dc31636-df95-47a0-bd79-c4174ff47ef1" class=""><mark class="highlight-blue"><strong>Light-weight deformable registration using adversarial learning with distilling knowledge</strong></mark></p><p id="154d5bc0-4478-8098-b912-ff6a0bbce272" class=""><span style="border-bottom:0.05em solid">Minh Tran*</span>, Tuong Do*, Huy Tran, Erman Tjiputra, Quang D Tran, Anh Nguyen, <em><code>*</code></em><em> indicates equal contribution</em></p><p id="148d5bc0-4478-805b-850e-e845c0b78fb7" class=""><em>IEEE Transactions on Medical Imaging 2022</em></p><p id="154d5bc0-4478-807d-b211-cf674f8e7c3b" class=""><mark class="highlight-blue"><a href="https://ieeexplore.ieee.org/abstract/document/9672098/">paper</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://github.com/aioz-ai/LDR_ALDK">code</a></mark></p><p id="154d5bc0-4478-80ae-b72a-ee5e379de8e3" class="">
</p></div></div><div id="7125dbd4-6e69-49a1-8c7c-e507ecc9983e" class="column-list"><div id="4a25b041-80ee-4240-86bd-4578476c2d07" style="width:25%" class="column"><figure id="c02c8e38-97c4-46ac-be82-d58886db5bf8" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Screenshot_2024-12-06_at_4.55.37_PM.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Screenshot_2024-12-06_at_4.55.37_PM.png"/></a></figure></div><div id="d0b0d094-ccdc-481d-ab3b-e21337b9d29d" style="width:75%" class="column"><p id="32a9cabd-94b3-4600-b453-541adfa75e4f" class=""><mark class="highlight-blue"><strong>Deep Federated Learning for Autonomous Driving</strong></mark></p><p id="154d5bc0-4478-80c1-bc6e-ec3894eb99c1" class="">Anh Nguyen, Tuong Do,<strong> </strong><span style="border-bottom:0.05em solid">Minh Tran</span>, Binh X Nguyen, Chien Duong, Tu Phan, Erman Tjiputra, Quang D Tran</p><p id="148d5bc0-4478-8022-ba6b-cc9745f7bf0d" class=""><em>IEEE Intelligent Vehicles Symposium, 2022</em></p><p id="154d5bc0-4478-80de-add9-d586e8d43fbf" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2110.05754">arXiv</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://github.com/aioz-ai/FADNet">code</a></mark></p><p id="154d5bc0-4478-80b9-aa25-f1e62455503e" class="">
</p></div></div><div id="a87d295d-0fbd-4645-a1be-c8a2b3510080" class="column-list"><div id="9d15c500-206a-4b50-abcc-50ae6b6b8718" style="width:25%" class="column"><figure id="b76a57ab-844d-450c-a42b-f0303d0a593c" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%206.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%206.png"/></a></figure><p id="154d5bc0-4478-8032-8151-ea138e91b560" class="">
</p></div><div id="0236b322-22f0-4be4-a318-fbc04531edaa" style="width:75%" class="column"><p id="2be1137d-5649-4934-a572-d75c569bd254" class=""><mark class="highlight-blue"><strong>SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data</strong></mark></p><p id="154d5bc0-4478-80bb-977e-f9bd32f95f6e" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Loi Ly, Binh-Son Hua, Ngan Le</p><p id="148d5bc0-4478-8023-b80a-fd1565047e9b" class=""><em>IEEE International Symposium on Biomedical Imaging 2022 (</em><em><strong>Oral Presentation)</strong></em></p><p id="154d5bc0-4478-806b-962b-d3c347bbdc09" class=""><mark class="highlight-blue"><a href="https://arxiv.org/abs/2201.05905">arXiv</a></mark><mark class="highlight-blue">  </mark></p></div></div><div id="6bb3bd0b-cdac-43a0-8300-fe33c7f460d5" class="column-list"><div id="c3d93486-babc-4bde-a8fb-9b4166fa73cd" style="width:12.5%" class="column"><p id="6a2cf47c-bb36-44a6-a3b4-0bcc140d0e25" class=""><mark class="highlight-gray"><em>2021</em></mark></p></div><div id="a5bbbe21-b409-48bb-bc4c-1d4d6de712db" style="width:87.5%" class="column"><hr id="d5432c06-7e24-4935-8edb-0aecc80986f7"/></div></div><div id="b6081743-059a-404f-85be-687bbb4de5fa" class="column-list"><div id="eaf1ad82-ff0d-48fd-a0d4-78442b0ba265" style="width:25%" class="column"><figure id="12fd3168-1863-496c-ad77-c4d711fbf95a" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%207.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%207.png"/></a></figure><p id="378521a2-04d9-4e19-ad03-8ca2855a492c" class="">
</p></div><div id="e1f3740c-5822-47a8-bbf8-297b30cef34c" style="width:75%" class="column"><p id="4d6deff2-3ca0-4992-99bf-9ad52758e94f" class=""><mark class="highlight-blue"><strong>Multiple meta-model quantifying for medical visual question answering</strong></mark></p><p id="154d5bc0-4478-8024-886a-c7b6100f701f" class="">Tuong Do, Binh X Nguyen, Erman Tjiputra, <span style="border-bottom:0.05em solid">Minh Tran</span>, Quang D Tran, Anh Nguyen</p><p id="148d5bc0-4478-8072-88dd-f48dae38d5a4" class=""><em>MICCAI 2021</em></p><p id="154d5bc0-4478-8076-9fd7-c5cb0bd6936a" class=""><mark class="highlight-blue"><a href="https://link.springer.com/chapter/10.1007/978-3-030-87240-3_7">paper</a></mark> /  <mark class="highlight-blue"><a href="https://github.com/aioz-ai/MICCAI21_MMQ">code</a></mark></p></div></div><div id="d5f187b4-b587-494e-b133-1e93f7343f72" class="column-list"><div id="961f90aa-d51f-4794-8c8b-9b795dc432ae" style="width:12.5%" class="column"><p id="322613d5-924e-49c0-b3d4-f588eec5fd56" class=""><mark class="highlight-gray"><em>2020</em></mark></p></div><div id="c6fafe32-7191-4e42-8d25-70f17b5d16e8" style="width:87.5%" class="column"><hr id="c526ec15-60b5-4204-8a24-43bad1c104a9"/></div></div><div id="febd8cf0-aa4b-4eb1-a0dd-424867017edc" class="column-list"><div id="d0ba842b-b4a2-4e74-9107-654cd820491f" style="width:25%" class="column"><figure id="4f2dd227-ad52-494e-bccf-43bb9e34eeff" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%208.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%208.png"/></a></figure></div><div id="9ae1e7db-d289-483c-9532-d6c82e7c0860" style="width:75%" class="column"><p id="951d7c88-88c2-4f3c-ae16-a19b93d93591" class=""><mark class="highlight-blue"><strong>BEETLEBOT: Indoor Self-Driving Delivery Robot</strong></mark></p><p id="154d5bc0-4478-803c-b7f3-f382b1ba8471" class="">AIOZ AI</p><p id="154d5bc0-4478-8020-b810-d18ccb6ce0cb" class=""><mark class="highlight-blue"><a href="https://beetle.aioz.io/">project page</a></mark></p><p id="154d5bc0-4478-80f4-9554-f97ba81ec072" class="">
</p></div></div><div id="1737b7a2-39fe-4542-8af6-457fc2e4ea61" class="column-list"><div id="c9bd675a-3c23-4a9f-9f27-351b2aa91352" style="width:25%" class="column"><figure id="1953dd51-6c53-41a5-b919-23faeb168027" class="image" style="text-align:left"><a href="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%209.png"><img style="width:132px" src="Minh%20Tran%209f03ab0587574ca9a431096d79735717/Untitled%209.png"/></a></figure><p id="00dd005f-b41f-4fe9-9fdf-e0b426a44557" class="">
</p></div><div id="d8617a98-f651-4293-99e1-c96dda342dee" style="width:75%" class="column"><p id="2ca5335c-75ff-457b-9265-fe5a6b57c3c9" class=""><mark class="highlight-blue"><strong>Mobile Robot Planner with Low-cost Cameras Using Deep Reinforcement Learning</strong></mark></p><p id="154d5bc0-4478-8083-84c9-f9c62ef6a91f" class=""><span style="border-bottom:0.05em solid">Minh Tran</span>, Ngoc Q Ly</p><p id="148d5bc0-4478-80be-84a5-ef36c92b840f" class=""><em>IEEE NICS 2020</em></p><p id="154d5bc0-4478-80f7-9b30-e49082c990c1" class=""><mark class="highlight-blue"><a href="https://ieeexplore.ieee.org/abstract/document/9335852/">paper</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="https://github.com/trqminh/rl-mapless-navigation">code</a></mark><mark class="highlight-blue"> / </mark><mark class="highlight-blue"><a href="http://trqminh.github.io/pdfs/Bachelor_Dissertation.pdf">B.Sc. Dissertation</a></mark></p></div></div><hr id="000bb4a7-be78-43cf-b43a-1368bb1c357e"/><hr id="148d5bc0-4478-8081-8a3c-df69d38ac15e"/><h2 id="148d5bc0-4478-801f-948b-ebd29c0e88aa" class="">Activities</h2><ul id="148d5bc0-4478-801e-9128-c9792395d363" class="bulleted-list"><li style="list-style-type:disc"><strong>Reviewer</strong> at CVPR, ACCV, MICCAI, IJCNN, ICPR.</li></ul><ul id="148d5bc0-4478-806d-a803-c5ed82815ff3" class="bulleted-list"><li style="list-style-type:disc"><strong>Teaching assistant:</strong> Algorithm (CSCE 4133), Introduction to Artificial Intelligence (CSCE 4613).</li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>